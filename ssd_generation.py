import copy
import inspect
from typing import TYPE_CHECKING, List, Optional, Union

import einops
import torch
from transformers.generation.logits_process import LogitsProcessorList
from transformers.generation.stopping_criteria import StoppingCriteriaList
from transformers.utils import logging

if TYPE_CHECKING:
    from transformers.generation.streamers import BaseStreamer
    from transformers.generation.utils import GenerationMixin
    from transformers.modeling_utils import PreTrainedModel

logger = logging.get_logger(__name__)


def staged_assisted_decoding(
    self: "GenerationMixin",
    input_ids: torch.LongTensor,
    assistant_model: "PreTrainedModel",
    do_sample: bool = False,
    logits_processor: Optional[LogitsProcessorList] = None,
    logits_warper: Optional[LogitsProcessorList] = None,
    stopping_criteria: Optional[StoppingCriteriaList] = None,
    pad_token_id: Optional[int] = None,
    eos_token_id: Optional[Union[int, List[int]]] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    output_scores: Optional[bool] = None,
    return_dict_in_generate: Optional[bool] = None,
    synced_gpus: bool = False,
    streamer: Optional["BaseStreamer"] = None,
    **model_kwargs,
):
    """Override `transformers.generation.utils.GenerationMixin.assisted_decoding`
    to enable staged speculative decoding"""
    if self.config.is_encoder_decoder or assistant_model.config.is_encoder_decoder:
        raise ValueError(
            "encoder_decoder not supprted yet for staged_assisted_decoding"
        )
    # Assistant: initialize assistant-related variables
    num_assistant_tokens = assistant_model.generation_config.num_assistant_tokens
    topk_tokens = assistant_model.generation_config.topk_tokens

    # check if assistant model accepts encoder_outputs
    assistant_accepts_encoder_outputs = "encoder_outputs" in set(
        inspect.signature(assistant_model.forward).parameters.keys()
    )
    if assistant_accepts_encoder_outputs:
        raise ValueError(
            "encoder_outputs not supported yet for staged_assisted_decoding"
        )

    # init values
    logits_processor = (
        logits_processor if logits_processor is not None else LogitsProcessorList()
    )
    logits_warper = (
        logits_warper if logits_warper is not None else LogitsProcessorList()
    )
    stopping_criteria = (
        stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
    )
    pad_token_id = (
        pad_token_id
        if pad_token_id is not None
        else self.generation_config.pad_token_id
    )
    eos_token_id = (
        eos_token_id
        if eos_token_id is not None
        else self.generation_config.eos_token_id
    )
    if eos_token_id is not None and pad_token_id is None:
        raise ValueError(
            "If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
        )

    # other auxiliary variables
    max_len = stopping_criteria[0].max_length

    # Generate initial kv cache for model
    model_inputs = {
        "input_ids": input_ids[:, :-1],
        "attention_mask": model_kwargs["attention_mask"][:, :-1],
        "past_key_values": None,
    }
    model_outputs = self(**model_inputs, use_cache=True)
    model_past_key_values = model_outputs.past_key_values
    candidate_input_ids = input_ids

    while True:
        # Assistant: main logic start
        cur_len = input_ids.shape[-1]

        #  1. Forecast next N tokens using the assistant model. This `for` block can be replaced with a
        # `.generate()` call if we decide to add `past_key_values` as a possible output of generate, as we
        # need access to the assistant cache to secure strong speedups.

        # Generate initial kv cache for assistant model
        # currently need to rerun this every iteration because last token
        # is generated by model and does not have a corresponding kv cache
        # TODO (Rohan138): This is inefficient; find a better way!
        assistant_model_outputs = assistant_model(
            candidate_input_ids[:, :-1],
            past_key_values=model_kwargs.pop("assistant_past_key_values", None),
        )
        assistant_past_key_values = assistant_model_outputs.past_key_values

        # Build staged speculative decoding tree
        tree_input_ids = []
        tree_past_key_values = []

        tree_input_ids.append(
            einops.repeat(
                candidate_input_ids[:, -1:],
                "b ... -> (b k) ...",
                k=topk_tokens**num_assistant_tokens,
            )
        )
        tree_past_key_values.append(assistant_past_key_values)

        for depth in range(int(num_assistant_tokens)):
            assistant_model_outputs = assistant_model(
                candidate_input_ids[:, -1:],
                past_key_values=assistant_past_key_values,
            )

            # 1.2. greedily select the next candidate token
            if len(logits_processor) > 0:
                assistant_model_outputs.logits[:, -1, :] = logits_processor(
                    candidate_input_ids, assistant_model_outputs.logits[:, -1, :]
                )
            next_token_logits = assistant_model_outputs.logits[:, -1, :]
            if do_sample:
                next_token_logits = (
                    next_token_logits / assistant_model.config.temperature
                )

            _, next_token_ids = next_token_logits.topk(topk_tokens, dim=-1)

            candidate_input_ids = next_token_ids.reshape(-1, 1)

            expanded_past_key_values = []
            for layer in assistant_model_outputs.past_key_values:
                expanded_layer = []
                for item in layer:
                    expanded_item = einops.repeat(
                        item, "b ... -> (b k) ...", k=topk_tokens
                    )
                    expanded_layer.append(expanded_item)
                expanded_past_key_values.append(expanded_layer)
            assistant_past_key_values = expanded_past_key_values

            tree_input_ids.append(
                einops.repeat(
                    candidate_input_ids,
                    "b ... -> (b k) ...",
                    k=topk_tokens ** (num_assistant_tokens - depth - 1),
                )
            )
            tree_past_key_values.append(assistant_past_key_values)

            # 1.3. stop assistant generation on EOS
            # TODO (Rohan138): Handle eos_token better; right now we early stop
            # the depthwise generation of the whole tree if eos_token is found
            last_assistant_token_is_eos = (candidate_input_ids == eos_token_id).any()
            if last_assistant_token_is_eos:
                break

        tree_input_ids = torch.cat(tree_input_ids, dim=1)
        candidate_length = tree_input_ids.shape[1]

        # 2. Use the original model to obtain the next token logits given the candidate sequence. We obtain
        # `candidate_length + 1` relevant logits from this process: in the event that all candidates are correct,
        # we use this forward pass to also pick the subsequent logits in the original model.

        # 2.1. Prepare the model inputs
        candidate_kwargs = copy.copy(model_kwargs)
        candidate_kwargs = self._extend_attention_mask(
            candidate_kwargs, cur_len + candidate_length - 1
        )
        candidate_kwargs = self._extend_token_type_ids(
            candidate_kwargs, cur_len + candidate_length - 1
        )
        attention_mask = candidate_kwargs["attention_mask"]
        attention_mask = einops.repeat(
            attention_mask, "b ... -> (b k) ...", k=tree_input_ids.shape[0]
        )
        candidate_kwargs["attention_mask"] = attention_mask
        expanded_past_key_values = []
        for layer in model_past_key_values:
            expanded_layer = []
            for item in layer:
                expanded_item = einops.repeat(
                    item, "b ... -> (b k) ...", k=tree_input_ids.shape[0]
                )
                expanded_layer.append(expanded_item)
            expanded_past_key_values.append(expanded_layer)
        candidate_kwargs["past_key_values"] = expanded_past_key_values

        position_ids = torch.arange(
            cur_len, cur_len + candidate_length, device=input_ids.device
        )
        position_ids = einops.repeat(
            position_ids, "... -> k ...", k=tree_input_ids.shape[0]
        )
        candidate_kwargs["position_ids"] = position_ids

        # 2.2. Run a forward pass on the candidate sequence
        model_outputs = self(
            tree_input_ids,
            **candidate_kwargs,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )

        # 2.3. Process the new logits
        new_logits = model_outputs.logits

        # 3. Obtain the next tokens from the original model logits.
        if do_sample:
            new_logits = new_logits / self.generation_config.temperature
            dist = torch.distributions.Categorical(logits=new_logits)
            selected_tokens = dist.sample()
        else:
            selected_tokens = new_logits.argmax(dim=-1)

        # 4. Compare the selected tokens with the assistant tokens
        # 4.1. find valid indices where input token predicted by
        # assistant model is equal to previous output of model
        check_all = tree_input_ids[:, 1:] == selected_tokens[:, :-1]
        # hacky way to ensure that we "stop early" if False is found in check_all
        check_cum = torch.cumsum(check_all, dim=1) * torch.cumprod(check_all, dim=1)
        valid_indices = torch.max(check_cum, dim=1).values

        # 4.2 choose the best valid index and find number of matches
        chosen_index = valid_indices.argmax().item()
        n_matches = valid_indices[chosen_index].item()

        # 5. Add newly generated tokens to input_ids
        # 5.1. Ensure we don't generate beyond max_len or an eos token
        if last_assistant_token_is_eos and n_matches == candidate_length:
            n_matches -= 1
        n_matches = min(n_matches, max_len - cur_len - 1)

        # 5.2. Get the valid continuation, after the matching tokens
        valid_tokens = selected_tokens[None, chosen_index, : n_matches + 1]
        if streamer is not None:
            streamer.put(valid_tokens.cpu())
        input_ids = torch.cat((input_ids, valid_tokens), dim=-1)
        cur_len = input_ids.shape[-1]
        last_token = valid_tokens[:, -1:]

        # 5.3. Terminate due to max_len or an eos token
        if stopping_criteria(input_ids, None) or last_token == eos_token_id:
            break

        # 6. Update values for next iteration
        # 6.1. Update candidate_input_ids
        last_input_token = tree_input_ids[chosen_index, n_matches]
        last_input_token = last_input_token.reshape(1, 1)
        candidate_input_ids = torch.cat([last_input_token, last_token], dim=1)

        # 6.2. Update attention mask
        mask = model_kwargs["attention_mask"]
        mask = torch.cat([mask, mask.new_ones((mask.shape[0], n_matches))], dim=-1)
        model_kwargs["attention_mask"] = mask

        # 6.3. Update assistant_past_key_values
        tree_index = chosen_index // (topk_tokens ** (num_assistant_tokens - n_matches))
        chosen_past_key_values = []
        for layer in tree_past_key_values[n_matches]:
            chosen_layer = []
            for item in layer:
                chosen_item = item[None, tree_index, :, :]
                chosen_layer.append(chosen_item)
            chosen_past_key_values.append(chosen_layer)
        model_kwargs["assistant_past_key_values"] = chosen_past_key_values

        # 6.4. Update past_key_values
        chosen_model_past_key_values = []
        for layer in model_outputs.past_key_values:
            chosen_layer = []
            for item in layer:
                chosen_item = item[None, chosen_index, :, : cur_len - 1]
                chosen_layer.append(chosen_item)
            chosen_model_past_key_values.append(chosen_layer)
        model_past_key_values = chosen_model_past_key_values

        # 6.5. Adjust the max number of assistant tokens to use in the next iteration. This is a simple heuristic,
        # probably can be improved -- we want to balance the benefits of getting assistant tokens correct with the
        # cost of forecasting incorrect assistant tokens.
        if (
            assistant_model.generation_config.num_assistant_tokens_schedule
            == "heuristic"
        ):
            if n_matches == int(num_assistant_tokens):
                num_assistant_tokens += 2
            else:
                num_assistant_tokens = max(1, num_assistant_tokens - 1)

        # 6.6. Explicitly free up unused memory before next iteration
        torch.cuda.empty_cache()

    if streamer is not None:
        streamer.end()

    return input_ids
